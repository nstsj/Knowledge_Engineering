{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KE_семинар2: Knowledge Graphs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IgammGUr1S-Y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zJ_g-OET6Ak"
      },
      "source": [
        "# Семинар2 \n",
        "\n",
        "создадим датасет и построим из него граф знаний\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgammGUr1S-Y"
      },
      "source": [
        "## 1. собираем данные: <br>\n",
        "будем использовать Википедию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KphZ7rhm1Lun"
      },
      "source": [
        "! pip3 install wikipedia-api # ставим библиотеку для API Википедии"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ywY8mVU-Z7f"
      },
      "source": [
        "# imports\n",
        "import wikipediaapi # импорт либы API\n",
        "import pandas as pd # таблички данных\n",
        "import concurrent.futures # отвечает за многопоточность процессов\n",
        "from tqdm import tqdm # визуализация прогресс-баров"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tixISyvh-o3Y"
      },
      "source": [
        "# функция ищет нужные темы в Википедии и извлекает информацию из нужных страничек + внешние ссылки\n",
        "\n",
        "def wiki_scrape(topic_name, verbose=True):\n",
        "    def wiki_link(link):\n",
        "        try:\n",
        "            page = wiki_api.page(link)\n",
        "            if page.exists():\n",
        "                d = {'page': link, 'text': page.text, 'link': page.fullurl,\n",
        "                     'categories': list(page.categories.keys())}\n",
        "                return d\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    wiki_api = wikipediaapi.Wikipedia(language='en',\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "    page_name = wiki_api.page(topic_name)\n",
        "    if not page_name.exists():\n",
        "        print('page {} does not exist'.format(topic_name))\n",
        "        return\n",
        "    page_links = list(page_name.links.keys())\n",
        "    progress = tqdm(desc='Links Scraped', unit='', total=len(page_links)) if verbose else None\n",
        "    sources = [{'page': topic_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
        "                'categories': list(page_name.categories.keys())}]\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        future_link = {executor.submit(wiki_link, link): link for link in page_links}\n",
        "        for future in concurrent.futures.as_completed(future_link):\n",
        "            data = future.result()\n",
        "            progress.update(1) if verbose else None\n",
        "            if data:\n",
        "                sources.append(data)\n",
        "    progress.close() if verbose else None\n",
        "    namespaces = ('Wikipedia', 'Special', 'Talk', 'LyricWiki', 'File', 'MediaWiki',\n",
        "                  'Template', 'Help', 'User', 'Category talk', 'Portal talk')\n",
        "    sources = pd.DataFrame(sources)\n",
        "    sources = sources[(len(sources['text']) > 20)\n",
        "                      & ~(sources['page'].str.startswith(namespaces, na=True))]\n",
        "    sources['categories'] = sources.categories.apply(lambda x: [y[9:] for y in x])\n",
        "    sources['topic'] = topic_name\n",
        "    print('Wikipedia pages scraped:', len(sources))\n",
        "    return sources"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm2fztFp_t0_"
      },
      "source": [
        "# в аргументе задайте интересующую Вас тему (не ссылку на страничку!)\n",
        "wiki_data = wiki_scrape('List of The Simpsons guest stars')\n",
        "# wiki_data = wiki_scrape('Financial crisis of 2007–08')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYMI1oNPAD8U"
      },
      "source": [
        "# можно посмотреть на таблицу собранных данных\n",
        "wiki_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asIhw8dQBCoe"
      },
      "source": [
        "# Если Вы хотите работать с одной вики-страничкой вместо нескольких, используйте эту ф-ю\n",
        "import wikipediaapi\n",
        "import pandas as pd\n",
        "\n",
        "def wiki_page(page_name):\n",
        "    wiki_api = wikipediaapi.Wikipedia(language='en',\n",
        "                                      extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "    page_name = wiki_api.page(page_name)\n",
        "    if not page_name.exists():\n",
        "        print('page does not exist')\n",
        "        return\n",
        "    page_data = {'page': page_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
        "                 'categories': [[y[9:] for y in list(page_name.categories.keys())]]}\n",
        "    page_data_df = pd.DataFrame(page_data)\n",
        "    return page_data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_3lno_cCcwj"
      },
      "source": [
        "# wiki_page(\"The Simpsons Movie\")\n",
        "wiki_page(\"Financial crisis of 2007–08\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqWyb6pRC5L3"
      },
      "source": [
        "## 2. немного NLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uie4rXkEWT8"
      },
      "source": [
        "! pip3 install spacy==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mox3bh1RDsiw"
      },
      "source": [
        "! pip3 install neuralcoref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNiudriaCsGX"
      },
      "source": [
        "# нужные импорты\n",
        "import re # регулярки\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "import neuralcoref # разрешение кореференции\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm') # знакомая с прошлого семинара модель. Можно попробовать для другого языка\n",
        "# документация spacy https://spacy.io/usage/models/\n",
        "\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hSfI8QEAwR4"
      },
      "source": [
        "def entity_pairs(text, coref=True):\n",
        "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
        "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
        "    text = nlp(text)\n",
        "    if coref:\n",
        "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
        "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
        "    ent_pairs = list()\n",
        "    for sent in sentences:\n",
        "        sent = nlp(sent)\n",
        "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
        "        spans = spacy.util.filter_spans(spans)\n",
        "        with sent.retokenize() as retokenizer:\n",
        "            [retokenizer.merge(span) for span in spans]\n",
        "        dep = [token.dep_ for token in sent]\n",
        "        if (dep.count('obj')+dep.count('dobj'))==1 \\\n",
        "                and (dep.count('subj')+dep.count('nsubj'))==1:\n",
        "            for token in sent:\n",
        "                if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
        "                    subject = [w for w in token.head.lefts if w.dep_\n",
        "                               in ('subj', 'nsubj')]  # identify subject nodes\n",
        "                    if subject:\n",
        "                        subject = subject[0]\n",
        "                        # identify relationship by root dependency\n",
        "                        relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
        "                        if relation:\n",
        "                            relation = relation[0]\n",
        "                            # add adposition or particle to relationship\n",
        "                            if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
        "                                relation = ' '.join((str(relation),\n",
        "                                        str(relation.nbor(1))))\n",
        "                        else:\n",
        "                            relation = 'unknown'\n",
        "                        subject, subject_type = refine_ent(subject, sent)\n",
        "                        token, object_type = refine_ent(token, sent)\n",
        "                        ent_pairs.append([str(subject), str(relation), str(token),\n",
        "                                str(subject_type), str(object_type)])\n",
        "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
        "                          if not any(str(x) == '' for x in sublist)]\n",
        "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
        "                         'relation', 'object', 'subject_type',\n",
        "                         'object_type'])\n",
        "    print('Entity pairs extracted:', str(len(filtered_ent_pairs)))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def refine_ent(ent, sent):\n",
        "    unwanted_tokens = (\n",
        "        'PRON',  # pronouns\n",
        "        'PART',  # particle\n",
        "        'DET',  # determiner\n",
        "        'SCONJ',  # subordinating conjunction\n",
        "        'PUNCT',  # punctuation\n",
        "        'SYM',  # symbol\n",
        "        'X',  # other\n",
        "        )\n",
        "    ent_type = ent.ent_type_  # get entity type\n",
        "    if ent_type == '':\n",
        "        ent_type = 'NOUN_CHUNK'\n",
        "        ent = ' '.join(str(t.text) for t in\n",
        "                nlp(str(ent)) if t.pos_\n",
        "                not in unwanted_tokens and t.is_stop == False)\n",
        "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
        "        t = ''\n",
        "        for i in range(len(sent) - ent.i):\n",
        "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
        "                t += ' ' + str(ent.nbor(i))\n",
        "            else:\n",
        "                ent = t.strip()\n",
        "                break\n",
        "    return ent, ent_type"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXPabvMNRSCf"
      },
      "source": [
        "text = wiki_data.loc[0,'text']\n",
        "display(text, len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL9FXO_6Dojs"
      },
      "source": [
        "pairs = entity_pairs(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WApJBTQFUWIf"
      },
      "source": [
        "## 3. Построим граф \n",
        "будем использовать NetworkX, построим ориентированный мультиграф с вершинами отсортированными по degree centrality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X8pjTDHJXiA"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def draw_kg(pairs):\n",
        "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
        "            create_using=nx.MultiDiGraph())\n",
        "    node_deg = nx.degree(k_graph)\n",
        "    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n",
        "    plt.figure(num=None, figsize=(120, 90), dpi=80)\n",
        "    nx.draw_networkx(\n",
        "        k_graph,\n",
        "        node_size=[int(deg[1]) * 500 for deg in node_deg],\n",
        "        arrowsize=20,\n",
        "        linewidths=1.5,\n",
        "        pos=layout,\n",
        "        edge_color='red',\n",
        "        edgecolors='black',\n",
        "        node_color='white',\n",
        "        )\n",
        "    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n",
        "                  pairs['relation'].tolist()))\n",
        "    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n",
        "                                 font_color='red')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cuPSzjbMMCQ"
      },
      "source": [
        "draw_kg(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSwWrHWKMPiQ"
      },
      "source": [
        "# если граф нечитаемый попробуем изменить размер фигуры\n",
        "def filter_graph(pairs, node):\n",
        "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
        "            create_using=nx.MultiDiGraph())\n",
        "    edges = nx.dfs_successors(k_graph, node)\n",
        "    nodes = []\n",
        "    for k, v in edges.items():\n",
        "        nodes.extend([k])\n",
        "        nodes.extend(v)\n",
        "    subgraph = k_graph.subgraph(nodes)\n",
        "    layout = (nx.random_layout(k_graph))\n",
        "    nx.draw_networkx(\n",
        "        subgraph,\n",
        "        node_size=1000,\n",
        "        arrowsize=20,\n",
        "        linewidths=1.5,\n",
        "        pos=layout,\n",
        "        edge_color='red',\n",
        "        edgecolors='black',\n",
        "        node_color='white'\n",
        "        )\n",
        "    labels = dict(zip((list(zip(pairs.subject, pairs.object))),\n",
        "                    pairs['relation'].tolist()))\n",
        "    edges= tuple(subgraph.out_edges(data=False))\n",
        "    sublabels ={k: labels[k] for k in edges}\n",
        "    nx.draw_networkx_edge_labels(subgraph, pos=layout, edge_labels=sublabels,\n",
        "                                font_color='red')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55cpc2GbMYuM"
      },
      "source": [
        "filter_graph(pairs, 'Homer') #  в аргументе укажите любую интересную вершину"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ncxNgZcw_xY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThAGyDmSU5V8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}